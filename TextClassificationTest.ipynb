{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import nltk\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document,vocab):\n",
    "    words=(document)\n",
    "    feature=[]\n",
    "    for w in vocab:\n",
    "        if(w in words):\n",
    "            feature.append(int(words.count(w)))\n",
    "        else:\n",
    "            feature.append(int(0))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(filename,stop_words):\n",
    "\n",
    "    with open(filename, 'r',encoding='utf-8', errors='ignore') as content_file:\n",
    "\n",
    "        content = content_file.read()\n",
    "        l=[]\n",
    "        l=content.split()\n",
    "        \n",
    "        words=[]\n",
    "        punct = string.punctuation\n",
    "        fake=['newsgroups', 'rob', 'strom', 'subject', 're', 'socmotss', 'et', 'al', 'princeton', 'axes', 'matching', 'funds', 'boy']\n",
    "        \n",
    "        \n",
    "        for w in l:\n",
    "            if w not in stop_words and w not in fake and len(w)>1 and len(w)<=15 and w not in punct :\n",
    "                w=''.join(e for e in w if e.isalnum())\n",
    "                words.append(w.lower())\n",
    "        \n",
    "        \n",
    "            \n",
    "        x = 'xref'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = 'x'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = '>>'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "        \n",
    "        x = \"i'm\"\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = 'it'\n",
    "\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = 'path'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = 'it.'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = 'from'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = '0'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = 'mr .'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = '(or'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x = 'us'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = \"i'd\"\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='etc.'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='(and'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='>>the'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x=\"let's\"\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "        \n",
    "        x='l.'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='(see'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='**'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='>not'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x='is.'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = '(which'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x='>for'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x=':)'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "        x='(for'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "            \n",
    "        x = '|>'\n",
    "        while x in words:\n",
    "            words.remove(x)\n",
    "            \n",
    "      \n",
    "\n",
    "        counts={}\n",
    "        for p in words:\n",
    "            counts[p]=(l.count(p))\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVocab(subfolders):\n",
    "    vocab={}\n",
    "    for i in subfolders:\n",
    "        for filename in glob(os.path.join(i,'*')):\n",
    "            ct={}\n",
    "            stop_words=(stopwords.words(\"english\"))\n",
    "        \n",
    "            ct=function(filename,stop_words)\n",
    "            vocab.update(ct)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTrainingData(subfolders,features,istraining,path):\n",
    "    output=[]\n",
    "    subfolder=next(os.walk(path))[1]\n",
    "    j=0\n",
    "    for i in subfolders:\n",
    "        for filename in glob(os.path.join(i,'*')):\n",
    "                with open(filename, 'r',encoding='utf-8', errors='ignore') as content_file:\n",
    "        \n",
    "                    content = content_file.read()\n",
    "                    l=[]\n",
    "                    l=content.split()\n",
    "                    temp=find_features(l,features)\n",
    "                    if(istraining==True):\n",
    "                        temp.append(subfolder[j])\n",
    "                    output.append(temp)\n",
    "        j=j+1\n",
    "    return output\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PassDirectorynameTobuildData(subfolders,istraining,path):\n",
    "    vocab=makeVocab(subfolders)\n",
    "    vocab=nltk.FreqDist(vocab)\n",
    "\n",
    "    features=list(vocab.keys())[:3000]\n",
    "    #print(features)\n",
    "    data=makeTrainingData(subfolders,features,istraining,path)\n",
    "    if(istraining==True):\n",
    "        features.append('output')\n",
    "    data=np.array(data)\n",
    "    df=pd.DataFrame(data)\n",
    "    df.columns=features\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output = 0\n",
    "    features = dictionary[current_class].keys()\n",
    "    for j in range(len(features) - 1):\n",
    "        count_class_and_feature =  x[j]+1\n",
    "        possible_values_current_feature = dictionary[current_class][j] + 3000\n",
    "        \n",
    "        count_class =  possible_values_current_feature\n",
    "        p = np.log(count_class_and_feature) - np.log(count_class)\n",
    "        output = output + p\n",
    "    count_class = dictionary[current_class]['total_count']\n",
    "    total_count = dictionary['total_count']\n",
    "    class_prob = np.log(count_class) - np.log(total_count)\n",
    "    output = output + class_prob\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(x, dictionary):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == 'total_count'):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, x_test):\n",
    "    y_pred = []\n",
    "    for x in x_test:\n",
    "        x_class = predictSinglePoint(x, dictionary)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    output_name = data.columns[-1]\n",
    "    features = data.columns[0:-1]\n",
    "    counts = {}\n",
    "    counts[\"total_count\"] = len(data)\n",
    "    possible_outputs = set(data[output_name])\n",
    "    for output in possible_outputs:\n",
    "        counts[output] = {}\n",
    "        smallData = data[data[output_name] == output]\n",
    "        counts[output][\"total_count\"] = len(smallData)\n",
    "        for i in range(len(features)):\n",
    "            f = features[i]\n",
    "            possible_values = list(map(int, smallData[features[i]]))\n",
    "            counts[output][i] = sum(possible_values)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\this pc\\\\Desktop\\\\SUMMER\\\\Cognizance_Machine_Learning\\\\Text_Classification\\\\mini_newsgroups'\n",
    "subfolders = [f.path for f in os.scandir('C:\\\\Users\\\\this pc\\\\Desktop\\\\SUMMER\\\\Cognizance_Machine_Learning\\\\Text_Classification\\\\mini_newsgroups') if f.is_dir() ]\n",
    "df=PassDirectorynameTobuildData(subfolders,True,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='C:\\\\Users\\\\this pc\\\\Desktop\\\\SUMMER\\\\Cognizance_Machine_Learning\\\\Text_Classification\\\\mini_newsgroups'\n",
    "subfolders = [f.path for f in os.scandir('C:\\\\Users\\\\this pc\\\\Desktop\\\\SUMMER\\\\Cognizance_Machine_Learning\\\\Text_Classification\\\\mini_newsgroups') if f.is_dir() ]\n",
    "df1=PassDirectorynameTobuildData(subfolders,False,path)\n",
    "x_test=np.array(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['total_count', 'comp.os.ms-windows.misc', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'talk.politics.misc', 'misc.forsale', 'soc.religion.christian', 'sci.space', 'comp.windows.x', 'rec.sport.hockey', 'talk.politics.mideast', 'talk.politics.guns', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'sci.crypt', 'sci.med', 'talk.religion.misc', 'alt.atheism', 'comp.graphics', 'comp.sys.mac.hardware'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = fit(df)\n",
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary['sci.space'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "5       False\n",
       "6       False\n",
       "7       False\n",
       "8       False\n",
       "9       False\n",
       "10      False\n",
       "11      False\n",
       "12      False\n",
       "13      False\n",
       "14      False\n",
       "15      False\n",
       "16      False\n",
       "17      False\n",
       "18      False\n",
       "19      False\n",
       "20      False\n",
       "21      False\n",
       "22      False\n",
       "23      False\n",
       "24      False\n",
       "25      False\n",
       "26      False\n",
       "27      False\n",
       "28      False\n",
       "29      False\n",
       "        ...  \n",
       "1970    False\n",
       "1971    False\n",
       "1972    False\n",
       "1973    False\n",
       "1974    False\n",
       "1975    False\n",
       "1976    False\n",
       "1977    False\n",
       "1978    False\n",
       "1979    False\n",
       "1980    False\n",
       "1981    False\n",
       "1982    False\n",
       "1983    False\n",
       "1984    False\n",
       "1985    False\n",
       "1986    False\n",
       "1987    False\n",
       "1988    False\n",
       "1989    False\n",
       "1990    False\n",
       "1991    False\n",
       "1992    False\n",
       "1993    False\n",
       "1994    False\n",
       "1995    False\n",
       "1996    False\n",
       "1997    False\n",
       "1998    False\n",
       "1999    False\n",
       "Name: output, Length: 2000, dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(dictionary, x_test)\n",
    "(y_pred==df['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ac = accuracy_score(df['output'], y_pred)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INBUILT MULTINOMIALNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "x = df.loc[:, df.columns != 'output']\n",
    "y = df.output\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_in = clf.predict(x_test)\n",
    "#clf.score(x_test, y_test)\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred_in)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
